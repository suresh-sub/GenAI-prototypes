{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606ae299",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLMs for Sentiment Analysis with LoRA\n",
    "\n",
    "This work explores the efficient fine-tuning of language models for sentiment analysis using Low-Rank Adaptation (LoRA). \n",
    "- **distilgpt2**: A lightweight transformer model (82M parameters)\n",
    "\n",
    "The model is fine-tuned on the Rotten Tomatoes movie review dataset for binary sentiment classification on a NVIDIA RTX 3070.\n",
    "\n",
    "## Approach\n",
    "1. Load and prepare the Rotten Tomatoes dataset\n",
    "2. Configure and apply LoRA for parameter-efficient fine-tuning\n",
    "3. Train both models on sentiment classification\n",
    "4. Evaluate and compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df3150",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "*  Rotten Tomatoes dataset - contains movie reviews labeled as positive or negative sentiment.\n",
    "*  Initial exploratory data analysis - display dataset size, label distribution, and example reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe01261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training examples: 8530\n",
      "Test examples: 1066\n",
      "\n",
      "Label distribution:\n",
      "Training set: {1: 4265, 0: 4265}\n",
      "Test set: {1: 533, 0: 533}\n",
      "\n",
      "Example reviews (randomly selected):\n",
      "Review: more vaudeville show than well-constructed narrative , but on those terms it's inoffensive and actually rather sweet .\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Review: what makes the film special is the refreshingly unhibited enthusiasm that the people , in spite of clearly evident poverty and hardship , bring to their music .\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Review: ozpetek's effort has the scope and shape of an especially well-executed television movie .\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import random\n",
    "\n",
    "# Set global constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load Rotten Tomatoes dataset\n",
    "ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
    "ds_test = load_dataset(\"rotten_tomatoes\", split=\"test\")\n",
    "\n",
    "# Get test data\n",
    "test_texts = ds_test[\"text\"]\n",
    "test_labels = ds_test[\"label\"]\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"Training examples: {len(ds)}\")\n",
    "print(f\"Test examples: {len(ds_test)}\")\n",
    "\n",
    "# Show label distribution\n",
    "train_label_counts = Counter(ds['label'])\n",
    "test_label_counts = Counter(ds_test['label'])\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(f\"Training set: {dict(train_label_counts)}\")\n",
    "print(f\"Test set: {dict(test_label_counts)}\")\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\nExample reviews (randomly selected):\")\n",
    "random_indices = random.sample(range(len(ds)), 3)\n",
    "for i in random_indices:\n",
    "    sentiment = \"Positive\" if ds[i][\"label\"] == 1 else \"Negative\"\n",
    "    print(f\"Review: {ds[i]['text']}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc77f3",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Functions we'll use for predictions and evaluation.\n",
    "* `gen` function predicts the sentiment of a single review by prompting the model to generate just one token (expected to be \"positive\" or \"negative\").\n",
    "* `predict_sentiment` function processes multiple reviews, using a similar single-token generation approach to classify each review as positive (1), negative (0), or unknown (-1). It can also add a specific instruction for a \"baseline\" prediction.\n",
    "* `display_prediction_distribution` function then summarizes these predictions, showing the count and percentage for each sentiment category (positive, negative, and unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54dcde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sentiment predictions for a single review\n",
    "def gen(model, tokenizer, review):\n",
    "    prompt = f\"Review: {review} Sentiment:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1,  # Only generate one token after the prompt\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to predict sentiment on a dataset\n",
    "def predict_sentiment(model, tokenizer, reviews, baseline=False):\n",
    "    preds = []\n",
    "    for review in tqdm(reviews, desc=\"Predicting sentiment\"):\n",
    "        prompt = f\"Review: {review} Sentiment:\"\n",
    "        if baseline:\n",
    "            prompt = f'Instruction: Analyze the following movie review and determine its sentiment. If the review expresses a positive sentiment, reply with \"positive\". If the review expresses a negative sentiment, reply with \"negative\". Only reply with \"positive\" or \"negative\", and do not provide any additional explanation.\\n {prompt}'\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        completion = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        last_word = completion.strip().split()[-1].lower()\n",
    "        if last_word == \"positive\":\n",
    "            preds.append(1)\n",
    "        elif last_word == \"negative\":\n",
    "            preds.append(0)\n",
    "        else:\n",
    "            preds.append(-1)  # Unknown sentiment\n",
    "    return preds\n",
    "\n",
    "# Function to display prediction distribution\n",
    "def display_prediction_distribution(y_pred, name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Displays the distribution of prediction values with counts and percentages.\n",
    "    \n",
    "    Args:\n",
    "        y_pred: List or array of prediction values\n",
    "        name: Name of the model for display purposes\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Calculate counts for predictions\n",
    "    pred_counts = Counter(y_pred)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Prediction Distribution for {name}:\")\n",
    "    for value, count in sorted(pred_counts.items()):\n",
    "        if value == 1:\n",
    "            label = \"Positive\"\n",
    "        elif value == 0:\n",
    "            label = \"Negative\"\n",
    "        else:\n",
    "            label = \"Unknown\"\n",
    "        print(f\"{label} (value={value}): {count} samples ({count/len(y_pred):.1%})\")\n",
    "    \n",
    "    return pred_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6084594",
   "metadata": {},
   "source": [
    "**Fine-Tuning distilgpt2**\n",
    "\n",
    "We will fine-tune the smaller distilgpt2 model using LoRA. This model has approximately 82M parameters.\n",
    "* *Data Preparation*: The code first formats the Rotten Tomatoes dataset by transforming each review and its sentiment label into a \"Review: [text] Sentiment: [Positive/Negative]\" prompt-completion pair, which is the format the language model will learn to generate.\n",
    "* *Model Initialization (LoRA)*: It loads the pre-trained distilgpt2 model and configures it for fine-tuning using Low-Rank Adaptation (LoRA). LoRA is a parameter-efficient fine-tuning technique that adds small, trainable matrices to the pre-trained model, significantly reducing memory usage and training time while achieving comparable performance to full fine-tuning.\n",
    "* *Training Setup*: A Trainer object is set up with specific training arguments, including the output directory for logs, batch size, number of training epochs, and a data collator. The DataCollatorForLanguageModeling is crucial here as it prepares the tokenized data for causal language modeling, which is the objective of GPT-like models.\n",
    "* *Fine-tuning Execution*: The trainer.train() method then executes the fine-tuning process. During this phase, the model learns to predict the sentiment word (e.g., \"Positive\" or \"Negative\") given the review text, effectively adapting its pre-trained knowledge to the sentiment analysis task on the Rotten Tomatoes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae06e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suresh/git/personal-projects/.venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_19086/868983651.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6399' max='6399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6399/6399 03:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.895200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.725500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.722100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "\n",
    "BASE = \"distilgpt2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Format text + sentiment prompt completion\n",
    "def fmt(ex):\n",
    "    sentiment = \"Positive\" if ex[\"label\"] == 1 else \"Negative\"\n",
    "    prompt = f\"Review: {ex['text']} Sentiment:\"\n",
    "    target = f\" {sentiment}\"\n",
    "    return {\"text\": prompt + target}\n",
    "\n",
    "ds = ds.map(fmt)\n",
    "ds = ds.filter(lambda x: len(x[\"text\"].split()) < 120)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "def tok(ex):\n",
    "    return tokenizer(ex[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "ds = ds.map(tok, batched=False)\n",
    "\n",
    "# Load model + prepare LoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE).to(DEVICE)\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Train\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./tmp_distilgpt2\",  # Temporary dir just for logs\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    #logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    fp16=False,\n",
    "    label_names=[]\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()\n",
    "fine_tuned_model_distilgpt2 = model  # Keep in memory instead of saving\n",
    "print(\"âœ… Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73406f2d",
   "metadata": {},
   "source": [
    "### Testing Fine-tuned distilgpt2\n",
    "\n",
    "Let's evaluate our fine-tuned distilgpt2 model on a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c5b3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Sentiment Comparison: Base vs Fine-Tuned distilgpt2\n",
      "\n",
      "ðŸŽ¬ REVIEW:\n",
      "\"This movie was a thrilling rollercoaster from start to finish.\"\n",
      "\n",
      "ðŸ”¹ BASE MODEL: Review: This movie was a thrilling rollercoaster from start to finish. Sentiment: I\n",
      "ðŸ”¸ FINE-TUNED MODEL: Review: This movie was a thrilling rollercoaster from start to finish. Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "ðŸŽ¬ REVIEW:\n",
      "\"An absolute disaster performances.\"\n",
      "\n",
      "ðŸ”¹ BASE MODEL: Review: An absolute disaster performances. Sentiment: A\n",
      "ðŸ”¸ FINE-TUNED MODEL: Review: An absolute disaster performances. Sentiment: Negative\n",
      "--------------------------------------------------\n",
      "ðŸŽ¬ REVIEW:\n",
      "\"It's a bad film, and that's about it.\"\n",
      "\n",
      "ðŸ”¹ BASE MODEL: Review: It's a bad film, and that's about it. Sentiment: It\n",
      "ðŸ”¸ FINE-TUNED MODEL: Review: It's a bad film, and that's about it. Sentiment: Negative\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load base and fine-tuned models\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE).to(DEVICE).eval()\n",
    "ft = fine_tuned_model_distilgpt2.eval()  # Use in-memory model directly\n",
    "\n",
    "test_reviews = [\n",
    "    \"This movie was a thrilling rollercoaster from start to finish.\",\n",
    "    \"An absolute disaster performances.\",\n",
    "    \"It's a bad film, and that's about it.\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“Š Sentiment Comparison: Base vs Fine-Tuned distilgpt2\\n\")\n",
    "for rev in test_reviews:\n",
    "    print(f\"ðŸŽ¬ REVIEW:\\n\\\"{rev}\\\"\\n\")\n",
    "    print(\"ðŸ”¹ BASE MODEL:\", gen(base, tokenizer, rev))\n",
    "    print(\"ðŸ”¸ FINE-TUNED MODEL:\", gen(ft, tokenizer, rev))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439549b",
   "metadata": {},
   "source": [
    "This example clearly shows the  performance of the fine-tuned distilgpt2 model over its base version for sentiment analysis. \n",
    "*  Effective instruction tuning on a smaller model like DistilGPT2 using consumer grade GPUs within a mere 3 minutes and 29 seconds significantly lowers the barrier to entry for advanced NLP tasks.\n",
    "* Dramatic improvement shown by the fine-tuned model (correctly identifying \"Positive\" and \"Negative\" sentiment) compared to the base model's unhelpful completions (\"I,\" \"A,\" \"It\") demonstrates that even a small model can be highly effective at specific tasks when properly instruction-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e775d",
   "metadata": {},
   "source": [
    "**Calculate distilgpt2 prediction**\n",
    "\n",
    "Evaluates both the base and fine-tuned DistilGPT2 models on the test dataset for sentiment prediction. It's crucial for quantitatively demonstrating the performance improvement achieved through fine-tuning, highlighting how the fine-tuned model becomes specifically adept at the sentiment analysis task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d4e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating distilGPT2 Models ---\n",
      "Predicting with base distilGPT2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:08<00:00, 121.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Prediction Distribution for Base distilGPT2 model:\n",
      "Unknown (value=-1): 1052 samples (98.7%)\n",
      "Negative (value=0): 9 samples (0.8%)\n",
      "Positive (value=1): 5 samples (0.5%)\n",
      "Time taken: 8.75 seconds\n",
      "\n",
      "Predicting with fine-tuned distilGPT2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1066/1066 [00:09<00:00, 107.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Prediction Distribution for Fine-tuned distilGPT2 model:\n",
      "Negative (value=0): 622 samples (58.3%)\n",
      "Positive (value=1): 444 samples (41.7%)\n",
      "Time taken: 9.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate distilGPT2 models\n",
    "import time\n",
    "print(\"\\n--- Evaluating distilGPT2 Models ---\")\n",
    "\n",
    "print(\"Predicting with base distilGPT2 model...\")\n",
    "start = time.time()\n",
    "y_pred_base = predict_sentiment(base, tokenizer, test_texts, baseline=True)\n",
    "elapsed_base = time.time() - start\n",
    "display_prediction_distribution(y_pred_base, name=\"Base distilGPT2 model\")\n",
    "print(f\"Time taken: {elapsed_base:.2f} seconds\")\n",
    "\n",
    "print(\"\\nPredicting with fine-tuned distilGPT2 model...\")\n",
    "start = time.time()\n",
    "y_pred = predict_sentiment(ft, tokenizer, test_texts, baseline=False)\n",
    "elapsed_ft = time.time() - start\n",
    "display_prediction_distribution(y_pred, name=\"Fine-tuned distilGPT2 model\")\n",
    "print(f\"Time taken: {elapsed_ft:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a6b6a",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "* Fine-tuned model successfully classifies nearly all reviews with an appropriate postive or negative sentiment, in stark contrast to the base model which classified 98.7% of reviews as \"Unknown\".\n",
    "* Fine-tuning effectively transformed a general-purpose language model into a task-specific sentiment classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1427d207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned distilGPT2:\n",
      "Accuracy: 0.7570\n",
      "Precision: 0.8086\n",
      "Recall: 0.6735\n",
      "F1 Score: 0.7349\n"
     ]
    }
   ],
   "source": [
    "# Calculate distilGPT2 metrics for fine-tuned model\n",
    "\n",
    "\n",
    "acc = accuracy_score(test_labels, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, y_pred, average=\"binary\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFine-tuned distilGPT2:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae93c29",
   "metadata": {},
   "source": [
    "**Key Takeways**\n",
    "*  Fine-tuned DistilGPT2 model achieved an impressive 75.70% accuracy, with balanced precision (80.86%), recall (67.35%), and a strong F1-score (73.49%).\n",
    "* Outcome highlights that instruction tuning, especially with techniques like LoRA on smaller models, can be incredibly effective even with minimal resources and consumer-grade hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
